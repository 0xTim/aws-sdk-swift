// THIS FILE IS AUTOMATICALLY GENERATED. DO NOT EDIT.
/**
The MIT License (MIT)

Copyright (c) 2017 Yuki Takei(noppoMan)

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

*/

import Foundation
import Core

extension Kinesis {

    public struct CreateStreamInput: Serializable, Initializable {
        /// A name to identify the stream. The stream name is scoped to the AWS account used by the application that creates the stream. It is also scoped by region. That is, two streams in two different AWS accounts can have the same name, and two streams in the same AWS account but in two different regions can have the same name.
        var streamName: String = ""
        /// The number of shards that the stream will use. The throughput of the stream is a function of the number of shards; more shards are required for greater provisioned throughput. DefaultShardLimit;
        var shardCount: Int32 = 0

        public init() {}

        public init(streamName: String, shardCount: Int32) {
            self.streamName = streamName
            self.shardCount = shardCount
        }

    }

    public struct AddTagsToStreamInput: Serializable, Initializable {
        /// The set of key-value pairs to use to create the tags.
        var tags: [String: String] = [:]
        /// The name of the stream.
        var streamName: String = ""

        public init() {}

        public init(tags: [String: String], streamName: String) {
            self.tags = tags
            self.streamName = streamName
        }

    }

    public struct DescribeLimitsInput: Serializable, Initializable {

        public init() {}

    }

    public struct DescribeStreamInput: Serializable, Initializable {
        /// The name of the stream to describe.
        var streamName: String = ""
        /// The shard ID of the shard to start with.
        var exclusiveStartShardId: String? = nil
        /// The maximum number of shards to return in a single call. The default value is 100. If you specify a value greater than 100, at most 100 shards are returned.
        var limit: Int32? = nil

        public init() {}

        public init(streamName: String, exclusiveStartShardId: String? = nil, limit: Int32? = nil) {
            self.streamName = streamName
            self.exclusiveStartShardId = exclusiveStartShardId
            self.limit = limit
        }

    }

    public struct UpdateShardCountOutput: Serializable, Initializable {
        /// The updated number of shards.
        var targetShardCount: Int32? = nil
        /// The current number of shards.
        var currentShardCount: Int32? = nil
        /// The name of the stream.
        var streamName: String? = nil

        public init() {}

        public init(targetShardCount: Int32? = nil, currentShardCount: Int32? = nil, streamName: String? = nil) {
            self.targetShardCount = targetShardCount
            self.currentShardCount = currentShardCount
            self.streamName = streamName
        }

    }

    public struct DecreaseStreamRetentionPeriodInput: Serializable, Initializable {
        /// The new retention period of the stream, in hours. Must be less than the current retention period.
        var retentionPeriodHours: Int32 = 0
        /// The name of the stream to modify.
        var streamName: String = ""

        public init() {}

        public init(retentionPeriodHours: Int32, streamName: String) {
            self.retentionPeriodHours = retentionPeriodHours
            self.streamName = streamName
        }

    }

    public struct IncreaseStreamRetentionPeriodInput: Serializable, Initializable {
        /// The new retention period of the stream, in hours. Must be more than the current retention period.
        var retentionPeriodHours: Int32 = 0
        /// The name of the stream to modify.
        var streamName: String = ""

        public init() {}

        public init(retentionPeriodHours: Int32, streamName: String) {
            self.retentionPeriodHours = retentionPeriodHours
            self.streamName = streamName
        }

    }

    public struct Shard: Serializable, Initializable {
        /// The unique identifier of the shard within the stream.
        var shardId: String = ""
        /// The range of possible sequence numbers for the shard.
        var sequenceNumberRange: SequenceNumberRange = SequenceNumberRange()
        /// The shard ID of the shard adjacent to the shard's parent.
        var adjacentParentShardId: String? = nil
        /// The range of possible hash key values for the shard, which is a set of ordered contiguous positive integers.
        var hashKeyRange: HashKeyRange = HashKeyRange()
        /// The shard ID of the shard's parent.
        var parentShardId: String? = nil

        public init() {}

        public init(shardId: String, sequenceNumberRange: SequenceNumberRange, adjacentParentShardId: String? = nil, hashKeyRange: HashKeyRange, parentShardId: String? = nil) {
            self.shardId = shardId
            self.sequenceNumberRange = sequenceNumberRange
            self.adjacentParentShardId = adjacentParentShardId
            self.hashKeyRange = hashKeyRange
            self.parentShardId = parentShardId
        }

    }

    public struct PutRecordsOutput: Serializable, Initializable {
        /// The number of unsuccessfully processed records in a PutRecords request.
        var failedRecordCount: Int32? = nil
        /// An array of successfully and unsuccessfully processed record results, correlated with the request by natural ordering. A record that is successfully added to a stream includes SequenceNumber and ShardId in the result. A record that fails to be added to a stream includes ErrorCode and ErrorMessage in the result.
        var records: [PutRecordsResultEntry] = []

        public init() {}

        public init(failedRecordCount: Int32? = nil, records: [PutRecordsResultEntry]) {
            self.failedRecordCount = failedRecordCount
            self.records = records
        }

    }

    public struct DescribeStreamOutput: Serializable, Initializable {
        /// The current status of the stream, the stream ARN, an array of shard objects that comprise the stream, and whether there are more shards available.
        var streamDescription: StreamDescription = StreamDescription()

        public init() {}

        public init(streamDescription: StreamDescription) {
            self.streamDescription = streamDescription
        }

    }

    public struct EnhancedMonitoringOutput: Serializable, Initializable {
        /// Represents the list of all the metrics that would be in the enhanced state after the operation.
        var desiredShardLevelMetrics: [String]? = nil
        /// The name of the Amazon Kinesis stream.
        var streamName: String? = nil
        /// Represents the current state of the metrics that are in the enhanced state before the operation.
        var currentShardLevelMetrics: [String]? = nil

        public init() {}

        public init(desiredShardLevelMetrics: [String]? = nil, streamName: String? = nil, currentShardLevelMetrics: [String]? = nil) {
            self.desiredShardLevelMetrics = desiredShardLevelMetrics
            self.streamName = streamName
            self.currentShardLevelMetrics = currentShardLevelMetrics
        }

    }

    public struct PutRecordOutput: Serializable, Initializable {
        /// The sequence number identifier that was assigned to the put data record. The sequence number for the record is unique across all records in the stream. A sequence number is the identifier associated with every record put into the stream.
        var sequenceNumber: String = ""
        /// The shard ID of the shard where the data record was placed.
        var shardId: String = ""

        public init() {}

        public init(sequenceNumber: String, shardId: String) {
            self.sequenceNumber = sequenceNumber
            self.shardId = shardId
        }

    }

    public struct Record: Serializable, Initializable {
        /// The approximate time that the record was inserted into the stream.
        var approximateArrivalTimestamp: Date? = nil
        /// The unique identifier of the record in the stream.
        var sequenceNumber: String = ""
        /// Identifies which shard in the stream the data record is assigned to.
        var partitionKey: String = ""
        /// The data blob. The data in the blob is both opaque and immutable to the Amazon Kinesis service, which does not inspect, interpret, or change the data in the blob in any way. When the data blob (the payload before base64-encoding) is added to the partition key size, the total size must not exceed the maximum record size (1 MB).
        var data: Data = Data()

        public init() {}

        public init(approximateArrivalTimestamp: Date? = nil, sequenceNumber: String, partitionKey: String, data: Data) {
            self.approximateArrivalTimestamp = approximateArrivalTimestamp
            self.sequenceNumber = sequenceNumber
            self.partitionKey = partitionKey
            self.data = data
        }

    }

    public struct DisableEnhancedMonitoringInput: Serializable, Initializable {
        /// List of shard-level metrics to disable. The following are the valid shard-level metrics. The value "ALL" disables every metric.    IncomingBytes     IncomingRecords     OutgoingBytes     OutgoingRecords     WriteProvisionedThroughputExceeded     ReadProvisionedThroughputExceeded     IteratorAgeMilliseconds     ALL    For more information, see Monitoring the Amazon Kinesis Streams Service with Amazon CloudWatch in the Amazon Kinesis Streams Developer Guide.
        var shardLevelMetrics: [String] = []
        /// The name of the Amazon Kinesis stream for which to disable enhanced monitoring.
        var streamName: String = ""

        public init() {}

        public init(shardLevelMetrics: [String], streamName: String) {
            self.shardLevelMetrics = shardLevelMetrics
            self.streamName = streamName
        }

    }

    public struct HashKeyRange: Serializable, Initializable {
        /// The starting hash key of the hash key range.
        var startingHashKey: String = ""
        /// The ending hash key of the hash key range.
        var endingHashKey: String = ""

        public init() {}

        public init(startingHashKey: String, endingHashKey: String) {
            self.startingHashKey = startingHashKey
            self.endingHashKey = endingHashKey
        }

    }

    public struct DescribeLimitsOutput: Serializable, Initializable {
        /// The maximum number of shards.
        var shardLimit: Int32 = 0
        /// The number of open shards.
        var openShardCount: Int32 = 0

        public init() {}

        public init(shardLimit: Int32, openShardCount: Int32) {
            self.shardLimit = shardLimit
            self.openShardCount = openShardCount
        }

    }

    public struct PutRecordsInput: Serializable, Initializable {
        /// The records associated with the request.
        var records: [PutRecordsRequestEntry] = []
        /// The stream name associated with the request.
        var streamName: String = ""

        public init() {}

        public init(records: [PutRecordsRequestEntry], streamName: String) {
            self.records = records
            self.streamName = streamName
        }

    }

    public struct GetShardIteratorInput: Serializable, Initializable {
        /// The sequence number of the data record in the shard from which to start reading. Used with shard iterator type AT_SEQUENCE_NUMBER and AFTER_SEQUENCE_NUMBER.
        var startingSequenceNumber: String? = nil
        /// The timestamp of the data record from which to start reading. Used with shard iterator type AT_TIMESTAMP. A timestamp is the Unix epoch date with precision in milliseconds. For example, 2016-04-04T19:58:46.480-00:00 or 1459799926.480. If a record with this exact timestamp does not exist, the iterator returned is for the next (later) record. If the timestamp is older than the current trim horizon, the iterator returned is for the oldest untrimmed data record (TRIM_HORIZON).
        var timestamp: Date? = nil
        /// Determines how the shard iterator is used to start reading data records from the shard. The following are the valid Amazon Kinesis shard iterator types:  AT_SEQUENCE_NUMBER - Start reading from the position denoted by a specific sequence number, provided in the value StartingSequenceNumber.  AFTER_SEQUENCE_NUMBER - Start reading right after the position denoted by a specific sequence number, provided in the value StartingSequenceNumber.  AT_TIMESTAMP - Start reading from the position denoted by a specific timestamp, provided in the value Timestamp.  TRIM_HORIZON - Start reading at the last untrimmed record in the shard in the system, which is the oldest data record in the shard.  LATEST - Start reading just after the most recent record in the shard, so that you always read the most recent data in the shard.  
        var shardIteratorType: String = ""
        /// The name of the Amazon Kinesis stream.
        var streamName: String = ""
        /// The shard ID of the Amazon Kinesis shard to get the iterator for.
        var shardId: String = ""

        public init() {}

        public init(startingSequenceNumber: String? = nil, timestamp: Date? = nil, shardIteratorType: String, streamName: String, shardId: String) {
            self.startingSequenceNumber = startingSequenceNumber
            self.timestamp = timestamp
            self.shardIteratorType = shardIteratorType
            self.streamName = streamName
            self.shardId = shardId
        }

    }

    public struct ListTagsForStreamOutput: Serializable, Initializable {
        /// If set to true, more tags are available. To request additional tags, set ExclusiveStartTagKey to the key of the last tag returned.
        var hasMoreTags: Bool = false
        /// A list of tags associated with StreamName, starting with the first tag after ExclusiveStartTagKey and up to the specified Limit. 
        var tags: [Tag] = []

        public init() {}

        public init(hasMoreTags: Bool, tags: [Tag]) {
            self.hasMoreTags = hasMoreTags
            self.tags = tags
        }

    }

    public struct UpdateShardCountInput: Serializable, Initializable {
        /// The new number of shards.
        var targetShardCount: Int32 = 0
        /// The name of the stream.
        var streamName: String = ""
        /// The scaling type. Uniform scaling creates shards of equal size.
        var scalingType: String = ""

        public init() {}

        public init(targetShardCount: Int32, streamName: String, scalingType: String) {
            self.targetShardCount = targetShardCount
            self.streamName = streamName
            self.scalingType = scalingType
        }

    }

    public struct Tag: Serializable, Initializable {
        /// An optional string, typically used to describe or define the tag. Maximum length: 256 characters. Valid characters: Unicode letters, digits, white space, _ . / = + - % @
        var value: String? = nil
        /// A unique identifier for the tag. Maximum length: 128 characters. Valid characters: Unicode letters, digits, white space, _ . / = + - % @
        var key: String = ""

        public init() {}

        public init(value: String? = nil, key: String) {
            self.value = value
            self.key = key
        }

    }

    public struct SequenceNumberRange: Serializable, Initializable {
        /// The ending sequence number for the range. Shards that are in the OPEN state have an ending sequence number of null.
        var endingSequenceNumber: String? = nil
        /// The starting sequence number for the range.
        var startingSequenceNumber: String = ""

        public init() {}

        public init(endingSequenceNumber: String? = nil, startingSequenceNumber: String) {
            self.endingSequenceNumber = endingSequenceNumber
            self.startingSequenceNumber = startingSequenceNumber
        }

    }

    public struct PutRecordsRequestEntry: Serializable, Initializable {
        /// The data blob to put into the record, which is base64-encoded when the blob is serialized. When the data blob (the payload before base64-encoding) is added to the partition key size, the total size must not exceed the maximum record size (1 MB).
        var data: Data = Data()
        /// The hash value used to determine explicitly the shard that the data record is assigned to by overriding the partition key hash.
        var explicitHashKey: String? = nil
        /// Determines which shard in the stream the data record is assigned to. Partition keys are Unicode strings with a maximum length limit of 256 characters for each key. Amazon Kinesis uses the partition key as input to a hash function that maps the partition key and associated data to a specific shard. Specifically, an MD5 hash function is used to map partition keys to 128-bit integer values and to map associated data records to shards. As a result of this hashing mechanism, all data records with the same partition key map to the same shard within the stream.
        var partitionKey: String = ""

        public init() {}

        public init(data: Data, explicitHashKey: String? = nil, partitionKey: String) {
            self.data = data
            self.explicitHashKey = explicitHashKey
            self.partitionKey = partitionKey
        }

    }

    public struct GetRecordsInput: Serializable, Initializable {
        /// The maximum number of records to return. Specify a value of up to 10,000. If you specify a value that is greater than 10,000, GetRecords throws InvalidArgumentException.
        var limit: Int32? = nil
        /// The position in the shard from which you want to start sequentially reading data records. A shard iterator specifies this position using the sequence number of a data record in the shard.
        var shardIterator: String = ""

        public init() {}

        public init(limit: Int32? = nil, shardIterator: String) {
            self.limit = limit
            self.shardIterator = shardIterator
        }

    }

    public struct DeleteStreamInput: Serializable, Initializable {
        /// The name of the stream to delete.
        var streamName: String = ""

        public init() {}

        public init(streamName: String) {
            self.streamName = streamName
        }

    }

    public struct GetShardIteratorOutput: Serializable, Initializable {
        /// The position in the shard from which to start reading data records sequentially. A shard iterator specifies this position using the sequence number of a data record in a shard.
        var shardIterator: String? = nil

        public init() {}

        public init(shardIterator: String? = nil) {
            self.shardIterator = shardIterator
        }

    }

    public struct PutRecordInput: Serializable, Initializable {
        /// The name of the stream to put the data record into.
        var streamName: String = ""
        /// Determines which shard in the stream the data record is assigned to. Partition keys are Unicode strings with a maximum length limit of 256 characters for each key. Amazon Kinesis uses the partition key as input to a hash function that maps the partition key and associated data to a specific shard. Specifically, an MD5 hash function is used to map partition keys to 128-bit integer values and to map associated data records to shards. As a result of this hashing mechanism, all data records with the same partition key map to the same shard within the stream.
        var partitionKey: String = ""
        /// The hash value used to explicitly determine the shard the data record is assigned to by overriding the partition key hash.
        var explicitHashKey: String? = nil
        /// Guarantees strictly increasing sequence numbers, for puts from the same client and to the same partition key. Usage: set the SequenceNumberForOrdering of record n to the sequence number of record n-1 (as returned in the result when putting record n-1). If this parameter is not set, records will be coarsely ordered based on arrival time.
        var sequenceNumberForOrdering: String? = nil
        /// The data blob to put into the record, which is base64-encoded when the blob is serialized. When the data blob (the payload before base64-encoding) is added to the partition key size, the total size must not exceed the maximum record size (1 MB).
        var data: Data = Data()

        public init() {}

        public init(streamName: String, partitionKey: String, explicitHashKey: String? = nil, sequenceNumberForOrdering: String? = nil, data: Data) {
            self.streamName = streamName
            self.partitionKey = partitionKey
            self.explicitHashKey = explicitHashKey
            self.sequenceNumberForOrdering = sequenceNumberForOrdering
            self.data = data
        }

    }

    public struct PutRecordsResultEntry: Serializable, Initializable {
        /// The sequence number for an individual record result.
        var sequenceNumber: String? = nil
        /// The error message for an individual record result. An ErrorCode value of ProvisionedThroughputExceededException has an error message that includes the account ID, stream name, and shard ID. An ErrorCode value of InternalFailure has the error message "Internal Service Failure".
        var errorMessage: String? = nil
        /// The error code for an individual record result. ErrorCodes can be either ProvisionedThroughputExceededException or InternalFailure.
        var errorCode: String? = nil
        /// The shard ID for an individual record result.
        var shardId: String? = nil

        public init() {}

        public init(sequenceNumber: String? = nil, errorMessage: String? = nil, errorCode: String? = nil, shardId: String? = nil) {
            self.sequenceNumber = sequenceNumber
            self.errorMessage = errorMessage
            self.errorCode = errorCode
            self.shardId = shardId
        }

    }

    public struct ListTagsForStreamInput: Serializable, Initializable {
        /// The key to use as the starting point for the list of tags. If this parameter is set, ListTagsForStream gets all tags that occur after ExclusiveStartTagKey. 
        var exclusiveStartTagKey: String? = nil
        /// The name of the stream.
        var streamName: String = ""
        /// The number of tags to return. If this number is less than the total number of tags associated with the stream, HasMoreTags is set to true. To list additional tags, set ExclusiveStartTagKey to the last key in the response.
        var limit: Int32? = nil

        public init() {}

        public init(exclusiveStartTagKey: String? = nil, streamName: String, limit: Int32? = nil) {
            self.exclusiveStartTagKey = exclusiveStartTagKey
            self.streamName = streamName
            self.limit = limit
        }

    }

    public struct ListStreamsOutput: Serializable, Initializable {
        /// The names of the streams that are associated with the AWS account making the ListStreams request.
        var streamNames: [String] = []
        /// If set to true, there are more streams available to list.
        var hasMoreStreams: Bool = false

        public init() {}

        public init(streamNames: [String], hasMoreStreams: Bool) {
            self.streamNames = streamNames
            self.hasMoreStreams = hasMoreStreams
        }

    }

    public struct EnableEnhancedMonitoringInput: Serializable, Initializable {
        /// List of shard-level metrics to enable. The following are the valid shard-level metrics. The value "ALL" enables every metric.    IncomingBytes     IncomingRecords     OutgoingBytes     OutgoingRecords     WriteProvisionedThroughputExceeded     ReadProvisionedThroughputExceeded     IteratorAgeMilliseconds     ALL    For more information, see Monitoring the Amazon Kinesis Streams Service with Amazon CloudWatch in the Amazon Kinesis Streams Developer Guide.
        var shardLevelMetrics: [String] = []
        /// The name of the stream for which to enable enhanced monitoring.
        var streamName: String = ""

        public init() {}

        public init(shardLevelMetrics: [String], streamName: String) {
            self.shardLevelMetrics = shardLevelMetrics
            self.streamName = streamName
        }

    }

    public struct MergeShardsInput: Serializable, Initializable {
        /// The shard ID of the adjacent shard for the merge.
        var adjacentShardToMerge: String = ""
        /// The shard ID of the shard to combine with the adjacent shard for the merge.
        var shardToMerge: String = ""
        /// The name of the stream for the merge.
        var streamName: String = ""

        public init() {}

        public init(adjacentShardToMerge: String, shardToMerge: String, streamName: String) {
            self.adjacentShardToMerge = adjacentShardToMerge
            self.shardToMerge = shardToMerge
            self.streamName = streamName
        }

    }

    public struct RemoveTagsFromStreamInput: Serializable, Initializable {
        /// The name of the stream.
        var streamName: String = ""
        /// A list of tag keys. Each corresponding tag is removed from the stream.
        var tagKeys: [String] = []

        public init() {}

        public init(streamName: String, tagKeys: [String]) {
            self.streamName = streamName
            self.tagKeys = tagKeys
        }

    }

    public struct ListStreamsInput: Serializable, Initializable {
        /// The name of the stream to start the list with.
        var exclusiveStartStreamName: String? = nil
        /// The maximum number of streams to list.
        var limit: Int32? = nil

        public init() {}

        public init(exclusiveStartStreamName: String? = nil, limit: Int32? = nil) {
            self.exclusiveStartStreamName = exclusiveStartStreamName
            self.limit = limit
        }

    }

    public struct GetRecordsOutput: Serializable, Initializable {
        /// The next position in the shard from which to start sequentially reading data records. If set to null, the shard has been closed and the requested iterator will not return any more data. 
        var nextShardIterator: String? = nil
        /// The number of milliseconds the GetRecords response is from the tip of the stream, indicating how far behind current time the consumer is. A value of zero indicates record processing is caught up, and there are no new records to process at this moment.
        var millisBehindLatest: Int64? = nil
        /// The data records retrieved from the shard.
        var records: [Record] = []

        public init() {}

        public init(nextShardIterator: String? = nil, millisBehindLatest: Int64? = nil, records: [Record]) {
            self.nextShardIterator = nextShardIterator
            self.millisBehindLatest = millisBehindLatest
            self.records = records
        }

    }

    public struct SplitShardInput: Serializable, Initializable {
        /// The name of the stream for the shard split.
        var streamName: String = ""
        /// The shard ID of the shard to split.
        var shardToSplit: String = ""
        /// A hash key value for the starting hash key of one of the child shards created by the split. The hash key range for a given shard constitutes a set of ordered contiguous positive integers. The value for NewStartingHashKey must be in the range of hash keys being mapped into the shard. The NewStartingHashKey hash key value and all higher hash key values in hash key range are distributed to one of the child shards. All the lower hash key values in the range are distributed to the other child shard.
        var newStartingHashKey: String = ""

        public init() {}

        public init(streamName: String, shardToSplit: String, newStartingHashKey: String) {
            self.streamName = streamName
            self.shardToSplit = shardToSplit
            self.newStartingHashKey = newStartingHashKey
        }

    }

    public struct EnhancedMetrics: Serializable, Initializable {
        /// List of shard-level metrics. The following are the valid shard-level metrics. The value "ALL" enhances every metric.    IncomingBytes     IncomingRecords     OutgoingBytes     OutgoingRecords     WriteProvisionedThroughputExceeded     ReadProvisionedThroughputExceeded     IteratorAgeMilliseconds     ALL    For more information, see Monitoring the Amazon Kinesis Streams Service with Amazon CloudWatch in the Amazon Kinesis Streams Developer Guide.
        var shardLevelMetrics: [String]? = nil

        public init() {}

        public init(shardLevelMetrics: [String]? = nil) {
            self.shardLevelMetrics = shardLevelMetrics
        }

    }

    public struct StreamDescription: Serializable, Initializable {
        /// The Amazon Resource Name (ARN) for the stream being described.
        var streamARN: String = ""
        /// The current retention period, in hours.
        var retentionPeriodHours: Int32 = 0
        /// Represents the current enhanced monitoring settings of the stream.
        var enhancedMonitoring: [EnhancedMetrics] = []
        /// If set to true, more shards in the stream are available to describe.
        var hasMoreShards: Bool = false
        /// The shards that comprise the stream.
        var shards: [Shard] = []
        /// The approximate time that the stream was created.
        var streamCreationTimestamp: Date = Date()
        /// The name of the stream being described.
        var streamName: String = ""
        /// The current status of the stream being described. The stream status is one of the following states:    CREATING - The stream is being created. Amazon Kinesis immediately returns and sets StreamStatus to CREATING.    DELETING - The stream is being deleted. The specified stream is in the DELETING state until Amazon Kinesis completes the deletion.    ACTIVE - The stream exists and is ready for read and write operations or deletion. You should perform read and write operations only on an ACTIVE stream.    UPDATING - Shards in the stream are being merged or split. Read and write operations continue to work while the stream is in the UPDATING state.  
        var streamStatus: String = ""

        public init() {}

        public init(streamARN: String, retentionPeriodHours: Int32, enhancedMonitoring: [EnhancedMetrics], hasMoreShards: Bool, shards: [Shard], streamCreationTimestamp: Date, streamName: String, streamStatus: String) {
            self.streamARN = streamARN
            self.retentionPeriodHours = retentionPeriodHours
            self.enhancedMonitoring = enhancedMonitoring
            self.hasMoreShards = hasMoreShards
            self.shards = shards
            self.streamCreationTimestamp = streamCreationTimestamp
            self.streamName = streamName
            self.streamStatus = streamStatus
        }

    }

}